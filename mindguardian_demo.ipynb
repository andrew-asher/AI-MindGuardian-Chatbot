{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNOydvzVZbmj"
      },
      "source": [
        "## 1. Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzg-eVvqZbmk"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRFR-Q58Zbml"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l1Mm4TPZbml"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w_rHG8pZbml"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYm9JhT9Zbml"
      },
      "outputs": [],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-zCu8hsZbmm"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cp-ECs_Zbmm"
      },
      "outputs": [],
      "source": [
        "!pip install nomic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "truhetpdZbmm"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YepACQFtZbmm"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LConA3oZbmm"
      },
      "source": [
        "## 2. Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa-KXFbrZbmm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import os\n",
        "from groq import Groq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orSC2Qu8Zbmn"
      },
      "source": [
        "## 3. Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6m_if9jZbmn"
      },
      "outputs": [],
      "source": [
        "def remove_key_from_json_file(file_path, key_to_remove):\n",
        "    # Read the JSON data from the file\n",
        "    with open(file_path, 'r') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    # Check if json_data is a list of objects\n",
        "    if isinstance(json_data, list):\n",
        "        for obj in json_data:\n",
        "            if isinstance(obj, dict):\n",
        "                # Remove the key from each JSON object\n",
        "                obj.pop(key_to_remove, None)\n",
        "    elif isinstance(json_data, dict):\n",
        "        # If it's a single JSON object, remove the key directly\n",
        "        json_data.pop(key_to_remove, None)\n",
        "\n",
        "    # Write the updated JSON data back to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(json_data, file, indent=4)\n",
        "\n",
        "# Usage example\n",
        "file_path = 'Alexander_Street_shareGPT_2.0.json' #Psych8k dataset\n",
        "key_to_remove = 'instruction'\n",
        "\n",
        "remove_key_from_json_file(file_path, key_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1nYTjtSZbmn"
      },
      "source": [
        "## 4. Uploading Data to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqHhxdXZbmn"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=\"INSERT-PINECONE-API-KEY\") #INSERT YOUR PINECONE API KEY\n",
        "index_name = 'mindguardian'\n",
        "\n",
        "if index_name not in pc.list_indexes():\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=768,\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f52y5Rs2Zbmn"
      },
      "outputs": [],
      "source": [
        "# Load JSON data\n",
        "with open('Alexander_Street_shareGPT_2.0.json', 'r') as file:\n",
        "    dataset = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n54lWMnsZbmn"
      },
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgbsFtM4Zbmn"
      },
      "outputs": [],
      "source": [
        "# Function to create dataset embeddings and store in Pinecone\n",
        "def store_embedded_data_in_pinecone(dataset):\n",
        "    count=0\n",
        "    for data in dataset:\n",
        "        merged_text = data['input'] + \" \" + data['output']\n",
        "        embedding = embedding_model.encode(merged_text).tolist()\n",
        "        index.upsert([(str(count), embedding, data)])\n",
        "        count+=1\n",
        "\n",
        "# Store the embedded data in Pinecone\n",
        "store_embedded_data_in_pinecone(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwBOFf8UZbmn"
      },
      "source": [
        "## 5. Using Groq API for llama 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HptsumXNZbmn"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"INSERT-GROQ-API-KEY\" # INSERT YOUR GROQ API KEY\n",
        "\n",
        "# Get Groq API key\n",
        "groq_api_key = os.environ['GROQ_API_KEY']\n",
        "llm_model = 'llama-3.1-70b-versatile'\n",
        "# Initialize Groq Langchain chat object and conversation\n",
        "groq_chat = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=llm_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f71W7wSAZbmn"
      },
      "source": [
        "## 6. LLM Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctx1B8cFZbmn"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjjz9bMnZbmn"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key=\"INSERT-PINECONE-API-KEY\") #INSERT YOUR PINECONE API KEY\n",
        "index_name = 'mindguardian'\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68v5MIYhZbmn"
      },
      "outputs": [],
      "source": [
        "system_prompt = 'You are a expert mental health counseling chatbot named Mindguardian, You provide professional mental health counseling to users'\n",
        "conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
        "memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaBUmdTnZbmn"
      },
      "outputs": [],
      "source": [
        "# Function to query Pinecone\n",
        "def query_pinecone(user_query):\n",
        "    # Generate the query vector from the user's input\n",
        "    query_vector = embedding_model.encode(user_query).tolist()\n",
        "\n",
        "    # Query Pinecone for the top 5 similar vectors\n",
        "    response = index.query(vector=query_vector, top_k=5, include_metadata=True)\n",
        "    return response['matches']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxHWMoxlZbmn"
      },
      "outputs": [],
      "source": [
        "def query_llm(user_question,_):\n",
        "    context = query_pinecone(user_question)\n",
        "\n",
        "    # Construct a chat prompt template using various components\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=system_prompt\n",
        "            ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
        "\n",
        "            MessagesPlaceholder(\n",
        "                variable_name=\"chat_history\"\n",
        "            ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
        "\n",
        "            # Include the retrieved context in the prompt\n",
        "            SystemMessage(\n",
        "                content=f\"Use this context only if relevant to user query: {context}\"\n",
        "            ),\n",
        "\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"User query: {human_input}\"\n",
        "            ),  # This template is where the user's current input will be injected into the prompt.\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
        "    conversation = LLMChain(\n",
        "        llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
        "        prompt=prompt,  # The constructed prompt template.\n",
        "        verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
        "        memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
        "    )\n",
        "    # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
        "    response = conversation.predict(human_input=user_question)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKYsr4kVZbmn"
      },
      "source": [
        "## 7. Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ickVCOfdZbmn"
      },
      "outputs": [],
      "source": [
        "default_message = \"\"\"I'm MindGuardian, a mental health counseling chatbot. How can I help you?\"\"\"\n",
        "\n",
        "# Gradio Interface\n",
        "gradio_interface = gr.ChatInterface(\n",
        "        query_llm,\n",
        "        chatbot=gr.Chatbot(value=[[None, default_message]]),\n",
        "        textbox=gr.Textbox(placeholder=\"Type your query\", container=False, scale=7),\n",
        "        title=\"Mindguardian, a mental health counseling chatbot\",\n",
        "        #description=f\"\",\n",
        "        theme='gradio/base', # themes at https://huggingface.co/spaces/gradio/theme-gallery\n",
        "        retry_btn=None,\n",
        "        undo_btn=\"Delete Previous\",\n",
        "        clear_btn=\"Clear\",\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "gradio_interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvCJzpaVZbmn"
      },
      "outputs": [],
      "source": [
        "gradio_interface.close()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}